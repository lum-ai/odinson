odinson {

  # all data related to odinson is stored here
  dataDir = ${user.home}/data/odinson

  # path to text files
  textDir = ${odinson.dataDir}/text

  # path to serialized documents
  docsDir = ${odinson.dataDir}/docs

  # path to lucene index
  indexDir = ${odinson.dataDir}/index

  # how many search results to display per page
  pageSize = 20

  # the token attribute to use for display.
  # NOTE: this must be **stored** in the current index in order for it to be retrievable.
  # By default, this should be "raw"
  displayField = "raw"

  # should a precise totalHits be calculated per query
  computeTotalHits = true

  state {
    provider = "fastsql" // "sql" "file" "memory"
    fastsql {
      // See http://www.h2database.com/html/features.html.
      url = "jdbc:h2:mem:odinson" // memory
      // url = "jdbc:h2:file:./odinson.db" // file
    }
    sql {
      // See http://www.h2database.com/html/features.html.
      url = "jdbc:h2:mem:odinson" // memory
      // url = "jdbc:h2:file:./odinson.db" // file
    }
  }

  compiler {

    # fields available per token
    allTokenFields = [
      ${odinson.index.rawTokenField},
      ${odinson.index.wordTokenField},
      ${odinson.index.normalizedTokenField},
      ${odinson.index.lemmaTokenField},
      ${odinson.index.posTagTokenField},
      ${odinson.index.chunkTokenField},
      ${odinson.index.entityTokenField},
      ${odinson.index.incomingTokenField},
      ${odinson.index.outgoingTokenField},
    ]

    # the token field to be used when none is specified
    defaultTokenField = ${odinson.index.normalizedTokenField}

    sentenceLengthField = ${odinson.index.sentenceLengthField}

    dependenciesField = ${odinson.index.dependenciesField}

    incomingTokenField = ${odinson.index.incomingTokenField}

    outgoingTokenField = ${odinson.index.outgoingTokenField}

    # if we are using the normalizedTokenField as the default
    # then we should casefold the queries to the default field
    # so that they match
    aggressiveNormalizationToDefaultField = true

  }

  index {

    # the raw token
    rawTokenField = raw

    # the word itself
    wordTokenField = word

    # a normalized version of the token
    normalizedTokenField = norm

    # the normalized field will include values from the following fields
    addToNormalizedField = [
        ${odinson.index.rawTokenField},
        ${odinson.index.wordTokenField},
    ]

    lemmaTokenField = lemma

    posTagTokenField = tag

    chunkTokenField = chunk

    entityTokenField = entity

    incomingTokenField = incoming

    outgoingTokenField = outgoing

    dependenciesField = dependencies

    documentIdField = docId

    sentenceIdField = sentId

    sentenceLengthField = numWords

    maxNumberOfTokensPerSentence = 100

    // SortedDocValuesField can store values of at most 32766 bytes long.
    // See https://lucene.apache.org/core/6_6_0/core/org/apache/lucene/document/SortedDocValuesField.html
    sortedDocValuesFieldMaxSize = 32766

    // When indexing make sure to add the documents in the order of the external document id, so that the
    // results returned by queries will be ordered by external document id
    // WARNING: turning this on will slow the indexing process as documents will be parsed twice
    synchronizeOrderWithDocumentId = false

    // Sometimes there are tokens in documents which are incompatible with the way we use Lucene.
    // In those cases, we replace the token with this character (default: ï¿½).
    invalidCharacterReplacement = "\ufffd"

  }

}
