{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \uf0c1 This repository contains the code for Odinson, a powerful and highly optimized open-source framework for information extraction. Odinson is a rule-based information extraction framework, which couples a simple, yet powerful pattern language that can operate over multiple representations of text, with a runtime system that operates in near real time. In the Odinson query language, a single pattern may combine regular expressions over surface tokens with regular expressions over graphs such as (but not limited to) syntactic dependencies. To guarantee the rapid matching of these patterns, the framework indexes most of the necessary information for matching patterns, including directed graphs such as syntactic dependencies, into a custom Lucene index. Indexing minimizes the amount of expensive pattern matching that must take place at runtime. Odinson is designed to facilitate real-time (or near real-time) queries over surface, syntax, or both. The syntax is based on that of its predecessor language, Odin, but there are some key divergences, detailed here. Project Structure \uf0c1 Odinson consists of the following subprojects: core : the core odinson library extra : these are a few apps that we need but don't really belong in core , due to things like licensing issues In c4815a8 , the Odinson REST API was moved to a separate repository. The project is being partially rewritten before it is publicly released. License \uf0c1 Odinson is Apache License Version 2.0. There are some supplemental utilities in the extra subproject that depend on GPL, notably Stanford's CoreNLP . Citation \uf0c1 If you use Odinson, please cite the following: Valenzuela-Esc\u00e1rcega, M. A., Hahn-Powell, G., & Bell, D. (2020, May). Odinson: A fast rule-based information extraction framework. In Proceedings of The 12th Language Resources and Evaluation Conference (pp. 2183-2191). [pdf] Bibtex: @InProceedings{Valenzuela:2020, author = {Valenzuela-Esc\u00e1rcega, Marco A. and Hahn-Powell, Gus and Bell, Dane}, title = {{O}dinson: {A} fast rule-based information extraction framework}, booktitle = {{Proceedings of the 12th Language Resources and Evaluation Conference}}, month = {May}, year = {2020}, address = {Marseille, France}, publisher = {European Language Resources Association}, pages = {2183--2191}, abstract = {We present Odinson, a rule-based information extraction framework, which couples a simple yet powerful pattern language that can operate over multiple representations of text, with a runtime system that operates in near real time. In the Odinson query language, a single pattern may combine regular expressions over surface tokens with regular expressions over graphs such as syntactic dependencies. To guarantee the rapid matching of these patterns, our framework indexes most of the necessary information for matching patterns, including directed graphs such as syntactic dependencies, into a custom Lucene index. Indexing minimizes the amount of expensive pattern matching that must take place at runtime. As a result, the runtime system matches a syntax-based graph traversal in 2.8 seconds in a corpus of over 134 million sentences, nearly 150,000 times faster than its predecessor.}, url = {https://www.aclweb.org/anthology/2020.lrec-1.267} }","title":"Home"},{"location":"#overview","text":"This repository contains the code for Odinson, a powerful and highly optimized open-source framework for information extraction. Odinson is a rule-based information extraction framework, which couples a simple, yet powerful pattern language that can operate over multiple representations of text, with a runtime system that operates in near real time. In the Odinson query language, a single pattern may combine regular expressions over surface tokens with regular expressions over graphs such as (but not limited to) syntactic dependencies. To guarantee the rapid matching of these patterns, the framework indexes most of the necessary information for matching patterns, including directed graphs such as syntactic dependencies, into a custom Lucene index. Indexing minimizes the amount of expensive pattern matching that must take place at runtime. Odinson is designed to facilitate real-time (or near real-time) queries over surface, syntax, or both. The syntax is based on that of its predecessor language, Odin, but there are some key divergences, detailed here.","title":"Overview"},{"location":"#project-structure","text":"Odinson consists of the following subprojects: core : the core odinson library extra : these are a few apps that we need but don't really belong in core , due to things like licensing issues In c4815a8 , the Odinson REST API was moved to a separate repository. The project is being partially rewritten before it is publicly released.","title":"Project Structure"},{"location":"#license","text":"Odinson is Apache License Version 2.0. There are some supplemental utilities in the extra subproject that depend on GPL, notably Stanford's CoreNLP .","title":"License"},{"location":"#citation","text":"If you use Odinson, please cite the following: Valenzuela-Esc\u00e1rcega, M. A., Hahn-Powell, G., & Bell, D. (2020, May). Odinson: A fast rule-based information extraction framework. In Proceedings of The 12th Language Resources and Evaluation Conference (pp. 2183-2191). [pdf] Bibtex: @InProceedings{Valenzuela:2020, author = {Valenzuela-Esc\u00e1rcega, Marco A. and Hahn-Powell, Gus and Bell, Dane}, title = {{O}dinson: {A} fast rule-based information extraction framework}, booktitle = {{Proceedings of the 12th Language Resources and Evaluation Conference}}, month = {May}, year = {2020}, address = {Marseille, France}, publisher = {European Language Resources Association}, pages = {2183--2191}, abstract = {We present Odinson, a rule-based information extraction framework, which couples a simple yet powerful pattern language that can operate over multiple representations of text, with a runtime system that operates in near real time. In the Odinson query language, a single pattern may combine regular expressions over surface tokens with regular expressions over graphs such as syntactic dependencies. To guarantee the rapid matching of these patterns, our framework indexes most of the necessary information for matching patterns, including directed graphs such as syntactic dependencies, into a custom Lucene index. Indexing minimizes the amount of expensive pattern matching that must take place at runtime. As a result, the runtime system matches a syntax-based graph traversal in 2.8 seconds in a corpus of over 134 million sentences, nearly 150,000 times faster than its predecessor.}, url = {https://www.aclweb.org/anthology/2020.lrec-1.267} }","title":"Citation"},{"location":"contributing/","text":"Contributing \uf0c1 Thank you for your interest in helping to improve Odinson! If you're looking for a way to help, please take a look at our open issues . For those new to the project, you may want to take a look at any issues labeled good first issue . When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Open communication helps us to avoid duplication of effort. Before getting started, be sure to review our Pull request Process for a step-by-step explanation of how to get your changes reviewed and merged promptly. Finally, please note we have a code of conduct that we expect everyone to follow throughout all interactions with the project and its community. Pull Request Process \uf0c1 Before a pull request can be accepted, all automated CI checks must pass. These include unit tests, linting, and coverage tests. When adding new functionality, it is important to include tests as part of your contribution to ensure the behavior is as intended and is not lost in future updates. See our testing page for details of how to write and run unit tests. Before opening a new pull request, ensure that ... 1. You've added tests for any new functionality. 2. All tests pass locally via sbt test . 3. Linting is performed by running sbt scalafmtAll scalafmtCheckAll . - You may want to configure your code editor or IDE to format all code according the project style when saving . - For some formatting tips, please see the next section. 4. You've updated CHANGES.md with a description of your contribution. Formatting Tips \uf0c1 Automatic formatting is not always optimal. If you notice formatting that it is not just ugly or annoying for a particular section of code, but that significantly impedes understanding or hides errors that a different format would expose, consider these two options: 1. Wrap the section between format comments // format: off and // format: on . 2. Temporarily override configuration settings with scalafmt comments // scalafmt: {} . Please consider use of these options to be recommendations. They may be ruled out during the pull request process. You may want to search for formatting comments in the code to familiarize yourself with their limited usage. Here is one particularly good example for guidance: def fancyFormattedQuantifiers(min, max) = { // format: off case (Some(min), Some(max), _) if min > max => Fail case (None, maxOption, \"}\") => Pass(GreedyQuantifier( 0, maxOption)) case (Some(min), maxOption, \"}\") => Pass(GreedyQuantifier(min, maxOption)) case (None, maxOption, \"}?\") => Pass(LazyQuantifier ( 0, maxOption)) case (Some(min), maxOption, \"}?\") => Pass(LazyQuantifier (min, maxOption)) // format: on } Contributor Covenant Code of Conduct \uf0c1 Our Pledge \uf0c1 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \uf0c1 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \uf0c1 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \uf0c1 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \uf0c1 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dev AT lum DOT ai . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \uf0c1 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \uf0c1 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \uf0c1 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \uf0c1 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \uf0c1 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \uf0c1 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Contributing"},{"location":"contributing/#contributing","text":"Thank you for your interest in helping to improve Odinson! If you're looking for a way to help, please take a look at our open issues . For those new to the project, you may want to take a look at any issues labeled good first issue . When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Open communication helps us to avoid duplication of effort. Before getting started, be sure to review our Pull request Process for a step-by-step explanation of how to get your changes reviewed and merged promptly. Finally, please note we have a code of conduct that we expect everyone to follow throughout all interactions with the project and its community.","title":"Contributing"},{"location":"contributing/#pull-request-process","text":"Before a pull request can be accepted, all automated CI checks must pass. These include unit tests, linting, and coverage tests. When adding new functionality, it is important to include tests as part of your contribution to ensure the behavior is as intended and is not lost in future updates. See our testing page for details of how to write and run unit tests. Before opening a new pull request, ensure that ... 1. You've added tests for any new functionality. 2. All tests pass locally via sbt test . 3. Linting is performed by running sbt scalafmtAll scalafmtCheckAll . - You may want to configure your code editor or IDE to format all code according the project style when saving . - For some formatting tips, please see the next section. 4. You've updated CHANGES.md with a description of your contribution.","title":"Pull Request Process"},{"location":"contributing/#formatting-tips","text":"Automatic formatting is not always optimal. If you notice formatting that it is not just ugly or annoying for a particular section of code, but that significantly impedes understanding or hides errors that a different format would expose, consider these two options: 1. Wrap the section between format comments // format: off and // format: on . 2. Temporarily override configuration settings with scalafmt comments // scalafmt: {} . Please consider use of these options to be recommendations. They may be ruled out during the pull request process. You may want to search for formatting comments in the code to familiarize yourself with their limited usage. Here is one particularly good example for guidance: def fancyFormattedQuantifiers(min, max) = { // format: off case (Some(min), Some(max), _) if min > max => Fail case (None, maxOption, \"}\") => Pass(GreedyQuantifier( 0, maxOption)) case (Some(min), maxOption, \"}\") => Pass(GreedyQuantifier(min, maxOption)) case (None, maxOption, \"}?\") => Pass(LazyQuantifier ( 0, maxOption)) case (Some(min), maxOption, \"}?\") => Pass(LazyQuantifier (min, maxOption)) // format: on }","title":"Formatting Tips"},{"location":"contributing/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"contributing/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"contributing/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"contributing/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"contributing/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"contributing/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dev AT lum DOT ai . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"contributing/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"contributing/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"contributing/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"contributing/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"contributing/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"contributing/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"contributors/","text":"Contributors: \uf0c1 Marco Valenzuela-Esc\u00e1rcega , Gus Hahn-Powell , Dane Bell , Keith Alcock , Becky Sharp , George Barbosa , Robert Vacareanu , and Mihai Surdeanu","title":"Contributors"},{"location":"contributors/#contributors","text":"Marco Valenzuela-Esc\u00e1rcega , Gus Hahn-Powell , Dane Bell , Keith Alcock , Becky Sharp , George Barbosa , Robert Vacareanu , and Mihai Surdeanu","title":"Contributors:"},{"location":"docker/","text":"Using with Docker \uf0c1 To build docker images locally, run the following command via sbt : sbt dockerize NOTE : this depends on having OpenJDK 11 installed on your machine. We also publish images to dockerhub (see below for information on our docker images). Docker image for annotating text and indexing Odinson JSON documents \uf0c1 docker pull lumai/odinson-extras:latest See our repository for other tags. Annotating text using the docker image \uf0c1 docker run \\ --name=\"odinson-extras\" \\ -it \\ --rm \\ -e \"HOME=/app\" \\ -e \"JAVA_OPTS=-Dodinson.extra.processorType=CluProcessor\" \\ -v \"/path/to/data/odinson:/app/data/odinson\" \\ --entrypoint \"bin/annotate-text\" \\ \"lumai/odinson-extras:latest\" NOTE : Replace /path/to/data/odinson with the path to the directory containing a directory called text containing the .txt files you want to annotate. Compressed OdinsonDocument JSON will be written to a directory called docs under whatever you use for /path/to/data/odinson . Indexing documents using the docker image \uf0c1 docker run \\ --name=\"odinson-extras\" \\ -it \\ --rm \\ -e \"HOME=/app\" \\ -v \"/path/to/data/odinson:/app/data/odinson\" \\ --entrypoint \"bin/index-documents\" \\ \"lumai/odinson-extras:latest\" NOTE : Replace /path/to/data/odinson with the path to the directory containing docs . The index will be written to a directory called index under whatever you use for /path/to/data/odinson .","title":"Docker"},{"location":"docker/#using-with-docker","text":"To build docker images locally, run the following command via sbt : sbt dockerize NOTE : this depends on having OpenJDK 11 installed on your machine. We also publish images to dockerhub (see below for information on our docker images).","title":"Using with Docker"},{"location":"docker/#docker-image-for-annotating-text-and-indexing-odinson-json-documents","text":"docker pull lumai/odinson-extras:latest See our repository for other tags.","title":"Docker image for annotating text and indexing Odinson JSON documents"},{"location":"docker/#annotating-text-using-the-docker-image","text":"docker run \\ --name=\"odinson-extras\" \\ -it \\ --rm \\ -e \"HOME=/app\" \\ -e \"JAVA_OPTS=-Dodinson.extra.processorType=CluProcessor\" \\ -v \"/path/to/data/odinson:/app/data/odinson\" \\ --entrypoint \"bin/annotate-text\" \\ \"lumai/odinson-extras:latest\" NOTE : Replace /path/to/data/odinson with the path to the directory containing a directory called text containing the .txt files you want to annotate. Compressed OdinsonDocument JSON will be written to a directory called docs under whatever you use for /path/to/data/odinson .","title":"Annotating text using the docker image"},{"location":"docker/#indexing-documents-using-the-docker-image","text":"docker run \\ --name=\"odinson-extras\" \\ -it \\ --rm \\ -e \"HOME=/app\" \\ -v \"/path/to/data/odinson:/app/data/odinson\" \\ --entrypoint \"bin/index-documents\" \\ \"lumai/odinson-extras:latest\" NOTE : Replace /path/to/data/odinson with the path to the directory containing docs . The index will be written to a directory called index under whatever you use for /path/to/data/odinson .","title":"Indexing documents using the docker image"},{"location":"installation/","text":"Setup \uf0c1 This software has been tested with Java 1.8 and Scala 2.12.10, and is available through sbt and Maven central. To include into an existing sbt project, add this to your build.sbt file: libraryDependencies ++= { val odinsonVer = \"X.X.X\" Seq( \"ai.lum\" %% \"odinson-core\" % odinsonVer, \"ai.lum\" %% \"odinson-extra\" % odinsonVer ) } Or, if you're using Maven, add this to your pom.xml file: <dependency> <groupId>ai.lum</groupId> <artifactId>odinson-core_2.12</artifactId> <version>x.x.x</version> </dependency> How to compile \uf0c1 As this is a sbt project, you can compile the code with sbt compile , then you can run any of the main files with either sbt core/run or sbt extra/run , depending on the location of the desired runnable.","title":"Installation"},{"location":"installation/#setup","text":"This software has been tested with Java 1.8 and Scala 2.12.10, and is available through sbt and Maven central. To include into an existing sbt project, add this to your build.sbt file: libraryDependencies ++= { val odinsonVer = \"X.X.X\" Seq( \"ai.lum\" %% \"odinson-core\" % odinsonVer, \"ai.lum\" %% \"odinson-extra\" % odinsonVer ) } Or, if you're using Maven, add this to your pom.xml file: <dependency> <groupId>ai.lum</groupId> <artifactId>odinson-core_2.12</artifactId> <version>x.x.x</version> </dependency>","title":"Setup"},{"location":"installation/#how-to-compile","text":"As this is a sbt project, you can compile the code with sbt compile , then you can run any of the main files with either sbt core/run or sbt extra/run , depending on the location of the desired runnable.","title":"How to compile"},{"location":"shell/","text":"Odinson shell \uf0c1 One way of interacting with Odinson is through the shell. To launch: docker run \\ --name=\"odinson-extras\" \\ -it \\ --rm \\ -e \"HOME=/app\" \\ -v \"/path/to/data/odinson:/app/data/odinson\" \\ --entrypoint \"bin/shell\" \\ \"lumai/odinson-extras:latest\" NOTE : Replace /path/to/data/odinson with the path to the directory containing index (created via the IndexDocuments runnable). Shell Examples \uf0c1 We have made a few example queries to show how the system works. For this we used a collection of 8,479 scientific papers (or 1,105,737 sentences). Please note that the rapidity of the execution allows a user to dynamically develop these queries in real-time, immediately receiving feedback on the coverage and precision of the patterns at scale. Example of a surface pattern for extracting casual relations. \uf0c1 This example shows odinson applying a pattern over surface features (i.e., words) to extract mentions of causal relations. Note that Odinson was able to find 3,774 sentences that match the pattern in 0.18 seconds. Example of a doubly-anchored Hearst pattern to extract hypernymy (i.e., X isA Y ) \uf0c1 This example shows how Odinson can also use patterns over syntax. In this case it tries to find hypernym relations. It finds 10,562 matches in 0.37 seconds. Example of how a surface pattern can be extended (using syntax patterns) to extract additional contextual information. \uf0c1 This example shows how surface and syntax can be combined in a single pattern. This pattern finds 12 sentences that match in our corpus of 1,105,737 sentences. It does this in 0.01 seconds. Example of a causal pattern written over dependency syntax with a lexical trigger (i.e., cause ). \uf0c1 This example shows how we can match over different aspects of tokens, lemmas in this example. Note that the ability to utilize syntax helps with the precision of the extractions (as compared with the overly simple surface rule above). Odinson finds 5,489 matches in 0.18 seconds. Example of how more complex patterns can be developed, for example, to extract the polarity of a causal influence and a context in which it applies. \uf0c1 This is an example of a slightly more complex pattern. Odinson is able to apply it over our corpus and finds 228 matches in 0.04 seconds.","title":"Odinson Shell"},{"location":"shell/#odinson-shell","text":"One way of interacting with Odinson is through the shell. To launch: docker run \\ --name=\"odinson-extras\" \\ -it \\ --rm \\ -e \"HOME=/app\" \\ -v \"/path/to/data/odinson:/app/data/odinson\" \\ --entrypoint \"bin/shell\" \\ \"lumai/odinson-extras:latest\" NOTE : Replace /path/to/data/odinson with the path to the directory containing index (created via the IndexDocuments runnable).","title":"Odinson shell"},{"location":"shell/#shell-examples","text":"We have made a few example queries to show how the system works. For this we used a collection of 8,479 scientific papers (or 1,105,737 sentences). Please note that the rapidity of the execution allows a user to dynamically develop these queries in real-time, immediately receiving feedback on the coverage and precision of the patterns at scale.","title":"Shell Examples"},{"location":"shell/#example-of-a-surface-pattern-for-extracting-casual-relations","text":"This example shows odinson applying a pattern over surface features (i.e., words) to extract mentions of causal relations. Note that Odinson was able to find 3,774 sentences that match the pattern in 0.18 seconds.","title":"Example of a surface pattern for extracting casual relations."},{"location":"shell/#example-of-a-doubly-anchored-hearst-pattern-to-extract-hypernymy-ie-x-isa-y","text":"This example shows how Odinson can also use patterns over syntax. In this case it tries to find hypernym relations. It finds 10,562 matches in 0.37 seconds.","title":"Example of a doubly-anchored Hearst pattern to extract hypernymy (i.e., X isA Y)"},{"location":"shell/#example-of-how-a-surface-pattern-can-be-extended-using-syntax-patterns-to-extract-additional-contextual-information","text":"This example shows how surface and syntax can be combined in a single pattern. This pattern finds 12 sentences that match in our corpus of 1,105,737 sentences. It does this in 0.01 seconds.","title":"Example of how a surface pattern can be extended (using syntax patterns) to extract additional contextual information."},{"location":"shell/#example-of-a-causal-pattern-written-over-dependency-syntax-with-a-lexical-trigger-ie-cause","text":"This example shows how we can match over different aspects of tokens, lemmas in this example. Note that the ability to utilize syntax helps with the precision of the extractions (as compared with the overly simple surface rule above). Odinson finds 5,489 matches in 0.18 seconds.","title":"Example of a causal pattern written over dependency syntax with a lexical trigger (i.e., cause)."},{"location":"shell/#example-of-how-more-complex-patterns-can-be-developed-for-example-to-extract-the-polarity-of-a-causal-influence-and-a-context-in-which-it-applies","text":"This is an example of a slightly more complex pattern. Odinson is able to apply it over our corpus and finds 228 matches in 0.04 seconds.","title":"Example of how more complex patterns can be developed, for example, to extract the polarity of a causal influence and a context in which it applies."},{"location":"testing/","text":"How does Odinson testing work? \uf0c1 Odinson uses ScalaTest 3 as a testing solution, and Scoverage for coverage checking. To run the entire collection of Odinson tests, run: sbt test You can use sbt 'runOnly *NameOfTheClass' to run a single test. Testing structure \uf0c1 Core tests \uf0c1 Core tests are divided into 6 categories: events: for event rules foundations: for fundamental aspects of the Odinson system, e.g., the ExtractorEngine patterns: tests different matching patterns serialization traversals: for traversing graph fields util: utility methods When writing a new test, try a category that best fits it. Documents for testing \uf0c1 When testing rules or components, you will find yourself needed a testing sentence to use with one of your tests. Inside ai.lum.odinson.documentation.ExampleSentences you will find a collection of sentences written for previous tests. You can reuse those sentences if you want. If you need a visualization of an odinson sentence you can use OdinsonDocEditor . How to add a unit test \uf0c1 Assuming you are creating a new file, make sure you follow these guidelines: Your test file should have a unique name and start with Test followed by the rest of the name, camel-cased. Follow the scalatest documentation when writing tests. Aim to test a single functionality in each test. Be mindful when naming tests and only use it when necessary. Every test suite should extend ai.lum.odinson.utils.TestUtils.OdinsonTest . It is good practice to structure your tests to avoid unhelpful messages. For example, if you consistently assert that x should be (true) , when a test fails you will only know that \"false did not equal true\", instead of the real problem! Code coverage \uf0c1 We use Codecov as our code coverage solution. The code coverage is calculated whenever you open a PR to the master repository. You can check the code coverage locally running: sbt coverage test sbt CoverageReport Test example \uf0c1 // part of foundations package ai.lum.odinson.foundations ai.lum.odinson.utils.TestUtils.OdinsonTest import collection.mutable.Stack // extend BaseSpec class TestSomething extends OdinsonTest { // use a descriptive name \"A Stack\" should \"pop values in last-in-first-out order\" in { val stack = new Stack[Int] stack.push(1) stack.push(2) stack.pop() should be (2) stack.pop() should be (1) } }","title":"Testing"},{"location":"testing/#how-does-odinson-testing-work","text":"Odinson uses ScalaTest 3 as a testing solution, and Scoverage for coverage checking. To run the entire collection of Odinson tests, run: sbt test You can use sbt 'runOnly *NameOfTheClass' to run a single test.","title":"How does Odinson testing work?"},{"location":"testing/#testing-structure","text":"","title":"Testing structure"},{"location":"testing/#core-tests","text":"Core tests are divided into 6 categories: events: for event rules foundations: for fundamental aspects of the Odinson system, e.g., the ExtractorEngine patterns: tests different matching patterns serialization traversals: for traversing graph fields util: utility methods When writing a new test, try a category that best fits it.","title":"Core tests"},{"location":"testing/#documents-for-testing","text":"When testing rules or components, you will find yourself needed a testing sentence to use with one of your tests. Inside ai.lum.odinson.documentation.ExampleSentences you will find a collection of sentences written for previous tests. You can reuse those sentences if you want. If you need a visualization of an odinson sentence you can use OdinsonDocEditor .","title":"Documents for testing"},{"location":"testing/#how-to-add-a-unit-test","text":"Assuming you are creating a new file, make sure you follow these guidelines: Your test file should have a unique name and start with Test followed by the rest of the name, camel-cased. Follow the scalatest documentation when writing tests. Aim to test a single functionality in each test. Be mindful when naming tests and only use it when necessary. Every test suite should extend ai.lum.odinson.utils.TestUtils.OdinsonTest . It is good practice to structure your tests to avoid unhelpful messages. For example, if you consistently assert that x should be (true) , when a test fails you will only know that \"false did not equal true\", instead of the real problem!","title":"How to add a unit test"},{"location":"testing/#code-coverage","text":"We use Codecov as our code coverage solution. The code coverage is calculated whenever you open a PR to the master repository. You can check the code coverage locally running: sbt coverage test sbt CoverageReport","title":"Code coverage"},{"location":"testing/#test-example","text":"// part of foundations package ai.lum.odinson.foundations ai.lum.odinson.utils.TestUtils.OdinsonTest import collection.mutable.Stack // extend BaseSpec class TestSomething extends OdinsonTest { // use a descriptive name \"A Stack\" should \"pop values in last-in-first-out order\" in { val stack = new Stack[Int] stack.push(1) stack.push(2) stack.pop() should be (2) stack.pop() should be (1) } }","title":"Test example"},{"location":"troubleshooting/","text":"Index Not Found Exception / white.lock error while using a docker image \uf0c1 Error message: Error in custom provider, org.apache.lucene.index.IndexNotFoundException: no segments* file found in MMapDirectory@/app/data/odinson/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@68e2d03e: files: [write.lock] Run the following command and index the documents using a docker image as described here rm -rf /path/to/data/odinson/index/ NOTE : Replace /path/to/data/odinson with the path to the directory containing index . No such method error while using a docker image \uf0c1 Error message: 'void sun.misc.Unsafe.putInt(java.lang.Object, int, int)' Make sure you have java 11 installed (e.g., OpenJDK 11). Access Denied Exception while indexing with a docker image \uf0c1 Error message: Exception in thread \"main\" java.nio.file.AccessDeniedException: /app/data/odinson/index/write.lock chmod -R 777 /path/to/data/odinson NOTE : Replace /path/to/data/odinson with the path to the directory containing docs . You may need to use sudo at the beginning of this command. No repo found while running offline documentation with a docker image \uf0c1 Error message: Liquid Exception: No repo name found. Specify using PAGES_REPO_NWO environment variables, 'repository' in your configuration, or set up an 'origin' git remote pointing to your github.com repository. in /_layouts/default.html Add the following line in the docs/_config.yml : repository: 'lum-ai/odinson.git' `/favicon.ico' not found while running offline documentation via docker \uf0c1 When running locally, this error can likely be ignored as explained here","title":"Troubleshooting"},{"location":"troubleshooting/#index-not-found-exception-whitelock-error-while-using-a-docker-image","text":"Error message: Error in custom provider, org.apache.lucene.index.IndexNotFoundException: no segments* file found in MMapDirectory@/app/data/odinson/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@68e2d03e: files: [write.lock] Run the following command and index the documents using a docker image as described here rm -rf /path/to/data/odinson/index/ NOTE : Replace /path/to/data/odinson with the path to the directory containing index .","title":"Index Not Found Exception / white.lock error while using a docker image"},{"location":"troubleshooting/#no-such-method-error-while-using-a-docker-image","text":"Error message: 'void sun.misc.Unsafe.putInt(java.lang.Object, int, int)' Make sure you have java 11 installed (e.g., OpenJDK 11).","title":"No such method error while using a docker image"},{"location":"troubleshooting/#access-denied-exception-while-indexing-with-a-docker-image","text":"Error message: Exception in thread \"main\" java.nio.file.AccessDeniedException: /app/data/odinson/index/write.lock chmod -R 777 /path/to/data/odinson NOTE : Replace /path/to/data/odinson with the path to the directory containing docs . You may need to use sudo at the beginning of this command.","title":"Access Denied Exception while indexing with a docker image"},{"location":"troubleshooting/#no-repo-found-while-running-offline-documentation-with-a-docker-image","text":"Error message: Liquid Exception: No repo name found. Specify using PAGES_REPO_NWO environment variables, 'repository' in your configuration, or set up an 'origin' git remote pointing to your github.com repository. in /_layouts/default.html Add the following line in the docs/_config.yml : repository: 'lum-ai/odinson.git'","title":"No repo found while running offline documentation with a docker image"},{"location":"troubleshooting/#faviconico-not-found-while-running-offline-documentation-via-docker","text":"When running locally, this error can likely be ignored as explained here","title":"`/favicon.ico' not found while running offline documentation via docker"},{"location":"getting-started/documents/","text":"Annotating text \uf0c1 Before an Odinson index can be created, the text needs to be annotated. You may use your own annotation tools , as long as you convert your annotated output to Odinson Documents . However, we also provide an App for annotating free text and producing this format, which makes use of the clulab Processors library . Configuration \uf0c1 The configurations are specified in extra/src/main/resources/application.conf . First, decide what Processor you'd like to use to annotate the text by specifying a value for odinson.extra.processorType . Available options are FastNLPProcessor , and CluProcessor . For more information about these, see clulab Processors . Ensure odinson.textDir and odinson.docDir are set as intended. Text will be read from odinson.textDir , annotated, and serialized to odinson.docDir . NOTE : We recommend a directory structure where you will have a data folder with subdirs text , docs , and index . If you do this, you can simply specify odinson.dataDir = path/to/your/dataDir , and the subfolders will be handled. Memory Usage \uf0c1 Depending on the number and size of the documents you are annotating, this step can be memory intensive. We recommend you set aside at least 8g, but if you have more it will run faster. You can specify this through this command: export SBT_OPTS=\"-Xmx8g\" Command \uf0c1 sbt \"extra/runMain ai.lum.odinson.extra.AnnotateText\" This step may take time, highly dependent on the length of your documents and the size of your corpus.","title":"Creating Odinson Documents"},{"location":"getting-started/documents/#annotating-text","text":"Before an Odinson index can be created, the text needs to be annotated. You may use your own annotation tools , as long as you convert your annotated output to Odinson Documents . However, we also provide an App for annotating free text and producing this format, which makes use of the clulab Processors library .","title":"Annotating text"},{"location":"getting-started/documents/#configuration","text":"The configurations are specified in extra/src/main/resources/application.conf . First, decide what Processor you'd like to use to annotate the text by specifying a value for odinson.extra.processorType . Available options are FastNLPProcessor , and CluProcessor . For more information about these, see clulab Processors . Ensure odinson.textDir and odinson.docDir are set as intended. Text will be read from odinson.textDir , annotated, and serialized to odinson.docDir . NOTE : We recommend a directory structure where you will have a data folder with subdirs text , docs , and index . If you do this, you can simply specify odinson.dataDir = path/to/your/dataDir , and the subfolders will be handled.","title":"Configuration"},{"location":"getting-started/documents/#memory-usage","text":"Depending on the number and size of the documents you are annotating, this step can be memory intensive. We recommend you set aside at least 8g, but if you have more it will run faster. You can specify this through this command: export SBT_OPTS=\"-Xmx8g\"","title":"Memory Usage"},{"location":"getting-started/documents/#command","text":"sbt \"extra/runMain ai.lum.odinson.extra.AnnotateText\" This step may take time, highly dependent on the length of your documents and the size of your corpus.","title":"Command"},{"location":"getting-started/making_index/","text":"Indexing Odinson Documents \uf0c1 Once you have created Odinson Documents , you can create an Odinson index, the data structure that Odinson uses for executing queries. Configuration \uf0c1 Once again, you will specify the configurations in extra/src/main/resources/application.conf . Ensure odinson.docDir directory contains annotated ai.lum.odinson.Document s ( .json or .json.gz ). Ensure odinson.indexDir is pointing to where you want to create the index. Note again, if you are using the typical directory structure (see above), you can simply ensure that odinson.dataDir = path/to/your/dataDir and the other paths will be correct. Command \uf0c1 sbt \"extra/runMain ai.lum.odinson.extra.IndexDocuments\" While annotating can be time-consuming, the creation of the index should be relatively less so, though again it's dependent on the number and size of the documents.","title":"Creating an Index"},{"location":"getting-started/making_index/#indexing-odinson-documents","text":"Once you have created Odinson Documents , you can create an Odinson index, the data structure that Odinson uses for executing queries.","title":"Indexing Odinson Documents"},{"location":"getting-started/making_index/#configuration","text":"Once again, you will specify the configurations in extra/src/main/resources/application.conf . Ensure odinson.docDir directory contains annotated ai.lum.odinson.Document s ( .json or .json.gz ). Ensure odinson.indexDir is pointing to where you want to create the index. Note again, if you are using the typical directory structure (see above), you can simply ensure that odinson.dataDir = path/to/your/dataDir and the other paths will be correct.","title":"Configuration"},{"location":"getting-started/making_index/#command","text":"sbt \"extra/runMain ai.lum.odinson.extra.IndexDocuments\" While annotating can be time-consuming, the creation of the index should be relatively less so, though again it's dependent on the number and size of the documents.","title":"Command"},{"location":"getting-started/walkthrough/","text":"Walkthrough Example \uf0c1 As an example of a typical usage of Odinson, imagine that you have a collection of text documents, from which you would like to extract mentions of pet adoptions. (A more cheerful example than, say, bombing events!) For this example, we'll use a single text file: Sally loves dogs. Yesterday, Sally adopted a cat named Ajax. Step 1: Annotate the text \uf0c1 which is saved as data/pets/text/text_1.txt . The next step is to annotate the text and build an Odinson index, both of which are offline steps. For this, your config file ( extra/src/main/resources/application.conf ) should read: odinson.textDir = data/pets/text odinson.docDir = data/pets/docs odinson.indexDir = data/pets/index To create the annotated Odinson documents, run this command from the project root directory: sbt \"extra/runMain ai.lum.odinson.extra.AnnotateText\" The Odinson Document for the above text, created with one example Processor is here, but keep in mind that the fields you include in an Odinson Document are largely up to you! Document(6a2b13bf-515f-49fe-a4f3-b3a04aaf3bef,List(),ArraySeq(Sentence(4,List(TokensField(raw,WrappedArray(Sally, loves, dogs, .),true), TokensField(word,WrappedArray(Sally, loves, dogs, .),false), TokensField(tag,WrappedArray(NNP, VBZ, NNS, .),false), TokensField(lemma,WrappedArray(Sally, love, dog, .),false), TokensField(entity,WrappedArray(PERSON, O, O, O),false), TokensField(chunk,WrappedArray(B-NP, B-VP, B-NP, O),false), GraphField(dependencies,List((1,0,nsubj), (1,2,dobj), (1,3,punct)),Set(1),false))), Sentence(9,List(TokensField(raw,WrappedArray(Yesterday, ,, Sally, adopted, a, cat, named, Ajax, .),true), TokensField(word,WrappedArray(Yesterday, ,, Sally, adopted, a, cat, named, Ajax, .),false), TokensField(tag,WrappedArray(NN, ,, NNP, VBD, DT, NN, VBN, NNP, .),false), TokensField(lemma,WrappedArray(yesterday, ,, Sally, adopt, a, cat, name, Ajax, .),false), TokensField(entity,WrappedArray(DATE, O, PERSON, O, O, O, O, ORGANIZATION, O),false), TokensField(chunk,WrappedArray(B-NP, O, B-NP, B-VP, B-NP, I-NP, B-VP, B-NP, O),false), GraphField(dependencies,List((3,2,nsubj), (3,5,dobj), (3,8,punct), (3,0,nmod:tmod), (3,1,punct), (5,4,det), (5,6,acl), (6,7,xcomp)),Set(3),false))))) Step 2: Create the index \uf0c1 Then, to make the index, you can either run the provided app: sbt \"extra/runMain ai.lum.odinson.extra.IndexDocuments\" Or, if customization is needed, you can use these steps in your own code: import ai.lum.common.FileUtils._ import ai.lum.odinson.Document import ai.lum.odinson.lucene.index.OdinsonIndexWriter // Initialize the index writer val writer = OdinsonIndexWriter.fromConfig() // Gather the Document files to be indexed val wildcards = Seq(\"*.json\", \"*.json.gz\") val files = new File(\"data/pets/docs\").listFilesByWildcards(wildcards, recursive = true) // Iterate through the files, and add each to the index files.foreach(f => writer.addFile(f)) // Close the index writer.close() Step 3: Use the index for queries \uf0c1 Once the index is built, we can query it to get the information we are interested in. To do this, we first need to make an extractor engine, then pass it queries to apply. import ai.lum.odinson.{EventMatch, ExtractorEngine, NamedCapture, OdinsonMatch} import ai.lum.odinson.utils.DisplayUtils.displayMention // Initialize the extractor engine -- ensure that your config still has `odinson.indexDir` pointing // to where you wrote your index, here we were using data/pets/index val extractorEngine = ExtractorEngine.fromConfig() // Here we have a set of two rules, which will first find `Pet` mentions, and the find // `Adoption` Mentions. val rules = \"\"\" |rules: | - name: pets_type | type: basic | label: Pet # if found, will have the label \"Pet\" | priority: 1 # will run in the first round of extraction | pattern: | | [lemma=/cat|dog|bunny|fish/] | | - name: pets_adoption | type: event | label: Adoption | priority: 2 # will run in the second round of extraction, can reference priority 1 rules | pattern: | | trigger = [lemma=adopt] | adopter = >nsubj [] # note: we didn't specify the label, so any token will work | pet: Pet = >dobj [] \"\"\".stripMargin // Compile the rules into Extractors that will be used with the Index val extractors = extractorEngine.compileRuleString(rules) // Extract Mentions val mentions = extractorEngine.extractMentions(extractors) // Display the mentions mentions.foreach(displayMention(_, extractorEngine)) The output of the above code is: ------------------------------ Mention Text: adopted Label: Adoption Found By: pets_adoption Trigger: adopted Args: * pet [Pet]: cat * adopter [no label]: Sally ------------------------------ Mention Text: dogs Label: Pet Found By: pets_type ------------------------------ Mention Text: cat Label: Pet Found By: pets_type Additional example usage \uf0c1 For an example usage, please see the complete working example here . To run it from the command line, use: sbt extra/run and choose Example off the list. To use it, you will need to point to an Odinson index by specifying the correct path in application.conf . If you need help making an index, or setting up your config file, there is info here . The file containing the Odinson rules being used in this example is here , and the output is a json lines file, with one line per extraction. Please note that this example is meant to be illustrative only, and the output produced is not a true serialization of the extracted Mentions (i.e., only some attributes are included in the output).","title":"Walkthrough Example"},{"location":"getting-started/walkthrough/#walkthrough-example","text":"As an example of a typical usage of Odinson, imagine that you have a collection of text documents, from which you would like to extract mentions of pet adoptions. (A more cheerful example than, say, bombing events!) For this example, we'll use a single text file: Sally loves dogs. Yesterday, Sally adopted a cat named Ajax.","title":"Walkthrough Example"},{"location":"getting-started/walkthrough/#step-1-annotate-the-text","text":"which is saved as data/pets/text/text_1.txt . The next step is to annotate the text and build an Odinson index, both of which are offline steps. For this, your config file ( extra/src/main/resources/application.conf ) should read: odinson.textDir = data/pets/text odinson.docDir = data/pets/docs odinson.indexDir = data/pets/index To create the annotated Odinson documents, run this command from the project root directory: sbt \"extra/runMain ai.lum.odinson.extra.AnnotateText\" The Odinson Document for the above text, created with one example Processor is here, but keep in mind that the fields you include in an Odinson Document are largely up to you! Document(6a2b13bf-515f-49fe-a4f3-b3a04aaf3bef,List(),ArraySeq(Sentence(4,List(TokensField(raw,WrappedArray(Sally, loves, dogs, .),true), TokensField(word,WrappedArray(Sally, loves, dogs, .),false), TokensField(tag,WrappedArray(NNP, VBZ, NNS, .),false), TokensField(lemma,WrappedArray(Sally, love, dog, .),false), TokensField(entity,WrappedArray(PERSON, O, O, O),false), TokensField(chunk,WrappedArray(B-NP, B-VP, B-NP, O),false), GraphField(dependencies,List((1,0,nsubj), (1,2,dobj), (1,3,punct)),Set(1),false))), Sentence(9,List(TokensField(raw,WrappedArray(Yesterday, ,, Sally, adopted, a, cat, named, Ajax, .),true), TokensField(word,WrappedArray(Yesterday, ,, Sally, adopted, a, cat, named, Ajax, .),false), TokensField(tag,WrappedArray(NN, ,, NNP, VBD, DT, NN, VBN, NNP, .),false), TokensField(lemma,WrappedArray(yesterday, ,, Sally, adopt, a, cat, name, Ajax, .),false), TokensField(entity,WrappedArray(DATE, O, PERSON, O, O, O, O, ORGANIZATION, O),false), TokensField(chunk,WrappedArray(B-NP, O, B-NP, B-VP, B-NP, I-NP, B-VP, B-NP, O),false), GraphField(dependencies,List((3,2,nsubj), (3,5,dobj), (3,8,punct), (3,0,nmod:tmod), (3,1,punct), (5,4,det), (5,6,acl), (6,7,xcomp)),Set(3),false)))))","title":"Step 1: Annotate the text"},{"location":"getting-started/walkthrough/#step-2-create-the-index","text":"Then, to make the index, you can either run the provided app: sbt \"extra/runMain ai.lum.odinson.extra.IndexDocuments\" Or, if customization is needed, you can use these steps in your own code: import ai.lum.common.FileUtils._ import ai.lum.odinson.Document import ai.lum.odinson.lucene.index.OdinsonIndexWriter // Initialize the index writer val writer = OdinsonIndexWriter.fromConfig() // Gather the Document files to be indexed val wildcards = Seq(\"*.json\", \"*.json.gz\") val files = new File(\"data/pets/docs\").listFilesByWildcards(wildcards, recursive = true) // Iterate through the files, and add each to the index files.foreach(f => writer.addFile(f)) // Close the index writer.close()","title":"Step 2: Create the index"},{"location":"getting-started/walkthrough/#step-3-use-the-index-for-queries","text":"Once the index is built, we can query it to get the information we are interested in. To do this, we first need to make an extractor engine, then pass it queries to apply. import ai.lum.odinson.{EventMatch, ExtractorEngine, NamedCapture, OdinsonMatch} import ai.lum.odinson.utils.DisplayUtils.displayMention // Initialize the extractor engine -- ensure that your config still has `odinson.indexDir` pointing // to where you wrote your index, here we were using data/pets/index val extractorEngine = ExtractorEngine.fromConfig() // Here we have a set of two rules, which will first find `Pet` mentions, and the find // `Adoption` Mentions. val rules = \"\"\" |rules: | - name: pets_type | type: basic | label: Pet # if found, will have the label \"Pet\" | priority: 1 # will run in the first round of extraction | pattern: | | [lemma=/cat|dog|bunny|fish/] | | - name: pets_adoption | type: event | label: Adoption | priority: 2 # will run in the second round of extraction, can reference priority 1 rules | pattern: | | trigger = [lemma=adopt] | adopter = >nsubj [] # note: we didn't specify the label, so any token will work | pet: Pet = >dobj [] \"\"\".stripMargin // Compile the rules into Extractors that will be used with the Index val extractors = extractorEngine.compileRuleString(rules) // Extract Mentions val mentions = extractorEngine.extractMentions(extractors) // Display the mentions mentions.foreach(displayMention(_, extractorEngine)) The output of the above code is: ------------------------------ Mention Text: adopted Label: Adoption Found By: pets_adoption Trigger: adopted Args: * pet [Pet]: cat * adopter [no label]: Sally ------------------------------ Mention Text: dogs Label: Pet Found By: pets_type ------------------------------ Mention Text: cat Label: Pet Found By: pets_type","title":"Step 3: Use the index for queries"},{"location":"getting-started/walkthrough/#additional-example-usage","text":"For an example usage, please see the complete working example here . To run it from the command line, use: sbt extra/run and choose Example off the list. To use it, you will need to point to an Odinson index by specifying the correct path in application.conf . If you need help making an index, or setting up your config file, there is info here . The file containing the Odinson rules being used in this example is here , and the output is a json lines file, with one line per extraction. Please note that this example is meant to be illustrative only, and the output produced is not a true serialization of the extracted Mentions (i.e., only some attributes are included in the output).","title":"Additional example usage"},{"location":"queries/basic_queries/","text":"Basic queries \uf0c1 Odinson basic queries allow for specifying a condition for the start of a \"path\", a valid end of the path, and the traversals that are licensed for the path itself. These conditions can be specified in terms of token constraints (surface patterns), path constraints (graph traversals), or both. Surface pattens \uf0c1 An example of a surface pattern is shown here: [tag=/N.*/] and [lemma=dog] This pattern will match any occurrence in the corpus of a noun (as specified by the tag beginning with N) followed by and , and finally being followed immediately by a word whose lemma is dog . Named Captures \uf0c1 To capture aspects of the match, we can add a named capture . To do this, you need to specify the name of the capture and surround the portion of the pattern that is to be captured with (?<name> ... ) . For example, when this query: (?<animal> [tag=/N.*/]) and [lemma=dog] is applied to the sentence \"I like cats and dogs\", the system will find the mention \"cats and dogs\", and this mention would contain a named capture with the label animal containing \"cats\". Adding syntax through graph traversals \uf0c1 Here is an example of a pattern that captures a subject-verb-object relation involving phosphorylation : (?<controller> [entity=PROTEIN]) <nsubj phosphorylates >dobj (?<theme> [entity=PROTEIN]) This pattern will look for a sentence in which a token tagged as a PROTEIN (though a hypothetical NER component) is the subject of the verb \"phosphorylates\", and in which that same verb has a direct object which is also tagged as a PROTEIN. To put it another way, reading the pattern from left-to-right, Odinson will look for a token tagged as a PROTEIN, try to traverse backwards against an incoming nsubj dependency arc, land on \"phosphorylates\", and then traverse an outgoing dobj dependency arc to land on a token also tagged as a PROTEIN. If it finds such a sentence, the first PROTEIN will be extracted with the label controller , and the second will have the label theme (because of the named captures). Combining representations \uf0c1 Note that in Odinson, patterns can hop between surface and syntax representations arbitrarily often, as is done in this query: Jack and Jill <nsubj went >nmod_up [] to fetch >dobj >nmod_of water which has a successful match in the sentence \"Jack and Jill went up the hill to fetch a pail of water.\"","title":"Basic Queries"},{"location":"queries/basic_queries/#basic-queries","text":"Odinson basic queries allow for specifying a condition for the start of a \"path\", a valid end of the path, and the traversals that are licensed for the path itself. These conditions can be specified in terms of token constraints (surface patterns), path constraints (graph traversals), or both.","title":"Basic queries"},{"location":"queries/basic_queries/#surface-pattens","text":"An example of a surface pattern is shown here: [tag=/N.*/] and [lemma=dog] This pattern will match any occurrence in the corpus of a noun (as specified by the tag beginning with N) followed by and , and finally being followed immediately by a word whose lemma is dog .","title":"Surface pattens"},{"location":"queries/basic_queries/#named-captures","text":"To capture aspects of the match, we can add a named capture . To do this, you need to specify the name of the capture and surround the portion of the pattern that is to be captured with (?<name> ... ) . For example, when this query: (?<animal> [tag=/N.*/]) and [lemma=dog] is applied to the sentence \"I like cats and dogs\", the system will find the mention \"cats and dogs\", and this mention would contain a named capture with the label animal containing \"cats\".","title":"Named Captures"},{"location":"queries/basic_queries/#adding-syntax-through-graph-traversals","text":"Here is an example of a pattern that captures a subject-verb-object relation involving phosphorylation : (?<controller> [entity=PROTEIN]) <nsubj phosphorylates >dobj (?<theme> [entity=PROTEIN]) This pattern will look for a sentence in which a token tagged as a PROTEIN (though a hypothetical NER component) is the subject of the verb \"phosphorylates\", and in which that same verb has a direct object which is also tagged as a PROTEIN. To put it another way, reading the pattern from left-to-right, Odinson will look for a token tagged as a PROTEIN, try to traverse backwards against an incoming nsubj dependency arc, land on \"phosphorylates\", and then traverse an outgoing dobj dependency arc to land on a token also tagged as a PROTEIN. If it finds such a sentence, the first PROTEIN will be extracted with the label controller , and the second will have the label theme (because of the named captures).","title":"Adding syntax through graph traversals"},{"location":"queries/basic_queries/#combining-representations","text":"Note that in Odinson, patterns can hop between surface and syntax representations arbitrarily often, as is done in this query: Jack and Jill <nsubj went >nmod_up [] to fetch >dobj >nmod_of water which has a successful match in the sentence \"Jack and Jill went up the hill to fetch a pail of water.\"","title":"Combining representations"},{"location":"queries/event_queries/","text":"Event queries \uf0c1 Odinson also supports event queries , which are look for a trigger, and then one or more required (or optional) arguments. These arguments are defined in terms of their connection to the found trigger. As an example, consider this simple event rule that finds the subject and object of the verb cause : - name: example-event-rule label: Causal type: event priority: 1 pattern: | trigger = causes subject = >nsubj [] object = >dobj [] The result of applying this rule to a sentence such as \"Rain causes puddles\" is a Mention, which has a trigger ( causes ), and two arguments: a subject ( Rainfall ), and an object ( puddles ). Since we did not specify any labels for the type of the subject and object arguments, they will have the same label as the overall event ( Causal ). Priorities \uf0c1 One element of an event query is the priority . This is the order in which the rule(s) should be applied. This is useful when you wish to rely on the output of a previous rule in another rule. For example, in these two rules, we first find Person Mentions, and then extract Construction events: - name: person-rule label: Person type: basic priority: 1 pattern: | [Hamilton] - name: construction-rule label: Construction type: event priority: 2 pattern: | trigger = [lemma=build] subject: Person = >nsubj [] Here, the engine will first apply person-rule , then construction-rule , due to the order indicated with the priorities. By adding the type specification to the subject argument ( subject: Person ), we indicate that the subject of the trigger must be a previously found Person . However, if we instead want to allow the rule to match even if the found subject is not a Person , we can make use of promotion . Argument Promotion \uf0c1 Argument promotion allows us to extract nested events in the moment, without requiring them to have been previously found. This feature must be enabled, using the ^ , as shown here: - name: construction-rule label: Construction type: event priority: 2 pattern: | trigger = [lemma=build] subject: ^Person = >nsubj [] Here, if the subject was not already found as a Person , we will still extract the event, and we will assign the label Person to the found subject. Optional and Required Arguments \uf0c1 Event arguments can either be optional or required, where required is the default. An optional argument will be extracted if present, but will not prevent the event from succeeding if not. Required arguments, on the other hand, will cause the event query to fail if they are not found. Optional arguments are indicated with a ? , as with the second argument below. - name: construction-rule label: Construction type: event priority: 2 pattern: | trigger = [lemma=build] subject: Person = >nsubj [] structure: Building? = >dobj [] Quantifying arguments \uf0c1 Arguments can be quantified, using the quantifiers described here . For example: - name: lots-of-quantifiers label: Eating type: event priority: 2 pattern: | trigger = [eats] subject: Person{1,2} = >nsubj [] food: ^Dessert+ = >dobj tool: ^Utensil* = >nmod_with In order to succeed, this query must find an instance of the word eats with one or 2 Person mentions as the subject(s), and one or more Dessert mentions as food arguments. If available, it will also match 0 or more Utensil mentions as tool arguments.","title":"Event Queries"},{"location":"queries/event_queries/#event-queries","text":"Odinson also supports event queries , which are look for a trigger, and then one or more required (or optional) arguments. These arguments are defined in terms of their connection to the found trigger. As an example, consider this simple event rule that finds the subject and object of the verb cause : - name: example-event-rule label: Causal type: event priority: 1 pattern: | trigger = causes subject = >nsubj [] object = >dobj [] The result of applying this rule to a sentence such as \"Rain causes puddles\" is a Mention, which has a trigger ( causes ), and two arguments: a subject ( Rainfall ), and an object ( puddles ). Since we did not specify any labels for the type of the subject and object arguments, they will have the same label as the overall event ( Causal ).","title":"Event queries"},{"location":"queries/event_queries/#priorities","text":"One element of an event query is the priority . This is the order in which the rule(s) should be applied. This is useful when you wish to rely on the output of a previous rule in another rule. For example, in these two rules, we first find Person Mentions, and then extract Construction events: - name: person-rule label: Person type: basic priority: 1 pattern: | [Hamilton] - name: construction-rule label: Construction type: event priority: 2 pattern: | trigger = [lemma=build] subject: Person = >nsubj [] Here, the engine will first apply person-rule , then construction-rule , due to the order indicated with the priorities. By adding the type specification to the subject argument ( subject: Person ), we indicate that the subject of the trigger must be a previously found Person . However, if we instead want to allow the rule to match even if the found subject is not a Person , we can make use of promotion .","title":"Priorities"},{"location":"queries/event_queries/#argument-promotion","text":"Argument promotion allows us to extract nested events in the moment, without requiring them to have been previously found. This feature must be enabled, using the ^ , as shown here: - name: construction-rule label: Construction type: event priority: 2 pattern: | trigger = [lemma=build] subject: ^Person = >nsubj [] Here, if the subject was not already found as a Person , we will still extract the event, and we will assign the label Person to the found subject.","title":"Argument Promotion"},{"location":"queries/event_queries/#optional-and-required-arguments","text":"Event arguments can either be optional or required, where required is the default. An optional argument will be extracted if present, but will not prevent the event from succeeding if not. Required arguments, on the other hand, will cause the event query to fail if they are not found. Optional arguments are indicated with a ? , as with the second argument below. - name: construction-rule label: Construction type: event priority: 2 pattern: | trigger = [lemma=build] subject: Person = >nsubj [] structure: Building? = >dobj []","title":"Optional and Required Arguments"},{"location":"queries/event_queries/#quantifying-arguments","text":"Arguments can be quantified, using the quantifiers described here . For example: - name: lots-of-quantifiers label: Eating type: event priority: 2 pattern: | trigger = [eats] subject: Person{1,2} = >nsubj [] food: ^Dessert+ = >dobj tool: ^Utensil* = >nmod_with In order to succeed, this query must find an instance of the word eats with one or 2 Person mentions as the subject(s), and one or more Dessert mentions as food arguments. If available, it will also match 0 or more Utensil mentions as tool arguments.","title":"Quantifying arguments"},{"location":"queries/graph_traversals/","text":"Graph traversals \uf0c1 Queries can also incorporate graph traversals, most commonly used for traversing the dependency syntax graph. A graph traversal is encoded with two parts: - a direction , and - a label The direction that the edge (e.g., dependency) is traversed is encoded by placing a > (outgoing) or < (incoming) in front of the label. The edge/dependency labels, as strings, support regular expression notation (i.e., /nmod_.*/ ). The full string rules are provided here . So, for example, to traverse an incoming nsubj edge, you'd use: <nsubj and to traverse an outgoing dobj or xcomp you'd use: >/dobj|xcomp/ Wildcards \uf0c1 Odinson supports wildcards for graph traversals. They are: - << : any incoming edge - >> : any outgoing edge Quantifiers and Expansion \uf0c1 Like any other pattern component, graph traversals (as well as these wildcards) can be combined with quantifiers , e.g., >>{2,3} . Additionally, groups of graph traversals can be wrapped in parentheses and quantified. For example, to specify traversing an outgoing nsubj edge and optionally up to two conj_and edges, you can use: She saw >dobj [] (>conj_and []){,2} In the sentence \"She saw me and Julio,\" this will traverse the dobj and the following conj_and and extract Julio . However, if we want to use the quantified (and here, optional) graph traversals to expand the mention, we can indicate it with (?^ ... ) as follows: She saw >dobj (?^ [] >conj_and []){,2}) The left-hand side, (?^ , indicates where the expanded mention should begin, and the final ) indicate where it should stop. The above pattern, with the same sentence, would instead match me and Julio .","title":"Graph Traversals"},{"location":"queries/graph_traversals/#graph-traversals","text":"Queries can also incorporate graph traversals, most commonly used for traversing the dependency syntax graph. A graph traversal is encoded with two parts: - a direction , and - a label The direction that the edge (e.g., dependency) is traversed is encoded by placing a > (outgoing) or < (incoming) in front of the label. The edge/dependency labels, as strings, support regular expression notation (i.e., /nmod_.*/ ). The full string rules are provided here . So, for example, to traverse an incoming nsubj edge, you'd use: <nsubj and to traverse an outgoing dobj or xcomp you'd use: >/dobj|xcomp/","title":"Graph traversals"},{"location":"queries/graph_traversals/#wildcards","text":"Odinson supports wildcards for graph traversals. They are: - << : any incoming edge - >> : any outgoing edge","title":"Wildcards"},{"location":"queries/graph_traversals/#quantifiers-and-expansion","text":"Like any other pattern component, graph traversals (as well as these wildcards) can be combined with quantifiers , e.g., >>{2,3} . Additionally, groups of graph traversals can be wrapped in parentheses and quantified. For example, to specify traversing an outgoing nsubj edge and optionally up to two conj_and edges, you can use: She saw >dobj [] (>conj_and []){,2} In the sentence \"She saw me and Julio,\" this will traverse the dobj and the following conj_and and extract Julio . However, if we want to use the quantified (and here, optional) graph traversals to expand the mention, we can indicate it with (?^ ... ) as follows: She saw >dobj (?^ [] >conj_and []){,2}) The left-hand side, (?^ , indicates where the expanded mention should begin, and the final ) indicate where it should stop. The above pattern, with the same sentence, would instead match me and Julio .","title":"Quantifiers and Expansion"},{"location":"queries/metadata/","text":"Metadata Query Language \uf0c1 Sometimes when we perform queries in Odinson we are only interested in a subset of the results. Maybe we are only interested in extractions from documents written by a particular author, or published in a specific venue, or maybe we only care about documents published recently. Odinson can index metadata associated to each document, which can be used to filter the results of a query through a metadata filter query. Odinson supports two main types of metadata: numeric and textual . Numeric Metadata \uf0c1 Numeric metadata can be compared using the common comparison operators used in most programming languages: equals ( == ), not equals ( != ), less than ( < ), greater than ( > ), less than or equals ( <= ), and greater than or equals ( >= ). One of the elements being compared must be an indexed metadata field , and the other must be a numeric value , e.g., citations > 5 . These comparisons can be chained together, allowing us to express ranges in a more concise way, e.g., 1 < citations < 10 . Field type to use: ai.lum.odinson.NumberField Textual Metadata \uf0c1 From those comparison operators, textual metadata only supports the equals ( == ) and not equals ( != ) operators. Equals checks for exact matching, e.g., publisher == 'mit press' . Note that the textual content is wrapped in quotation marks. Sometimes exact textual matching can be too stringent, and a containment check can be more appropriate. This can be accomplished with the contains operator, e.g., venue contains 'language' . To specify that a metadata field should not contain a given text, you can use: venue not contains 'language' To make textual comparison more robust, we perform some normalization of the textual metadata fields. - unicode-aware case folding - NFKC unicode normalization - some unicode characters are transformed into ASCII equivalents (e.g., arrows, ligatures, etc.) - removal of diacritics Field type to use: ai.lum.odinson.TokensField (Note, assumes the text is tokenized.) Combining Filters \uf0c1 The metadata query language supports the and ( && ), or ( || ), and not ( ! ) operators to combine individual field constraints into a more complex filter, e.g., 1 < citations < 10 && venue contains 'language' . Dates \uf0c1 The metadata query language offers special support for dates using the date function, which returns a numeric representation of the date. As such, you can write queries against dates as you would with other numeric metadata. For example, date(2020, 'Jan', 1) < pub_date <= date(2020, 'Dec', 31) would return documents published in 2020. This provides a lot of flexibility when checking for specific dates, but since checking for years is a common use case, we provide a shortcut. The query presented above can also be expressed as pub_date.year == 2020 . Months can be expressed as their full names, common abbreviations, or number (i.e., \"August\", \"Aug\", or 8). Field type to use: ai.lum.odinson.DateField Nested Fields \uf0c1 Another capability of the metadata query language is its support for nested metadata fields. For example, authors are usually indexed as nested fields so that their first and last name are associated with each other, and not with other authors. To query nested fields, we need to specify the name of the field and the query to be performed on it. For example, if we had a document with two authors named Jane Smith and John Doe , then we could match this document with the queries author{first=='jane' && last=='smith'} or author{first=='john' && last =='doe'} , but not author{first=='jane' && last=='doe'} . Not all attributes of a nested field must be specified. For example, given the authors above, this would also match: author{first=='jane'} Field type to use: ai.lum.odinson.NestedField Regular Expressions \uf0c1 The metadata query language also supports Lucene regular expressions, such as: - author{first=='/j.*/' && last=='/d.*/'} - keywords contains '/bio.*/' Adding Metadata to Existing Odinson Documents \uf0c1 As of PR #319 , there are some utilities and an app to help you add metadata to an Odinson document. - there is an addMetadata method in ai.lum.odinson.Document that take a sequence of Fields and adds them as metadata. - there is now a MetadataWrapper case class that can serialize itself in a compatible json format - in the extra/ subproject, there is an app ( ai.lum.odinson.extra.AddMetadataToDocuments ) that will load a set of metadata json files and a set of Document json files, and add any found metadata to the corresponding Document (as indicated by the docID in the metadata file).","title":"Metadata Query Language"},{"location":"queries/metadata/#metadata-query-language","text":"Sometimes when we perform queries in Odinson we are only interested in a subset of the results. Maybe we are only interested in extractions from documents written by a particular author, or published in a specific venue, or maybe we only care about documents published recently. Odinson can index metadata associated to each document, which can be used to filter the results of a query through a metadata filter query. Odinson supports two main types of metadata: numeric and textual .","title":"Metadata Query Language"},{"location":"queries/metadata/#numeric-metadata","text":"Numeric metadata can be compared using the common comparison operators used in most programming languages: equals ( == ), not equals ( != ), less than ( < ), greater than ( > ), less than or equals ( <= ), and greater than or equals ( >= ). One of the elements being compared must be an indexed metadata field , and the other must be a numeric value , e.g., citations > 5 . These comparisons can be chained together, allowing us to express ranges in a more concise way, e.g., 1 < citations < 10 . Field type to use: ai.lum.odinson.NumberField","title":"Numeric Metadata"},{"location":"queries/metadata/#textual-metadata","text":"From those comparison operators, textual metadata only supports the equals ( == ) and not equals ( != ) operators. Equals checks for exact matching, e.g., publisher == 'mit press' . Note that the textual content is wrapped in quotation marks. Sometimes exact textual matching can be too stringent, and a containment check can be more appropriate. This can be accomplished with the contains operator, e.g., venue contains 'language' . To specify that a metadata field should not contain a given text, you can use: venue not contains 'language' To make textual comparison more robust, we perform some normalization of the textual metadata fields. - unicode-aware case folding - NFKC unicode normalization - some unicode characters are transformed into ASCII equivalents (e.g., arrows, ligatures, etc.) - removal of diacritics Field type to use: ai.lum.odinson.TokensField (Note, assumes the text is tokenized.)","title":"Textual Metadata"},{"location":"queries/metadata/#combining-filters","text":"The metadata query language supports the and ( && ), or ( || ), and not ( ! ) operators to combine individual field constraints into a more complex filter, e.g., 1 < citations < 10 && venue contains 'language' .","title":"Combining Filters"},{"location":"queries/metadata/#dates","text":"The metadata query language offers special support for dates using the date function, which returns a numeric representation of the date. As such, you can write queries against dates as you would with other numeric metadata. For example, date(2020, 'Jan', 1) < pub_date <= date(2020, 'Dec', 31) would return documents published in 2020. This provides a lot of flexibility when checking for specific dates, but since checking for years is a common use case, we provide a shortcut. The query presented above can also be expressed as pub_date.year == 2020 . Months can be expressed as their full names, common abbreviations, or number (i.e., \"August\", \"Aug\", or 8). Field type to use: ai.lum.odinson.DateField","title":"Dates"},{"location":"queries/metadata/#nested-fields","text":"Another capability of the metadata query language is its support for nested metadata fields. For example, authors are usually indexed as nested fields so that their first and last name are associated with each other, and not with other authors. To query nested fields, we need to specify the name of the field and the query to be performed on it. For example, if we had a document with two authors named Jane Smith and John Doe , then we could match this document with the queries author{first=='jane' && last=='smith'} or author{first=='john' && last =='doe'} , but not author{first=='jane' && last=='doe'} . Not all attributes of a nested field must be specified. For example, given the authors above, this would also match: author{first=='jane'} Field type to use: ai.lum.odinson.NestedField","title":"Nested Fields"},{"location":"queries/metadata/#regular-expressions","text":"The metadata query language also supports Lucene regular expressions, such as: - author{first=='/j.*/' && last=='/d.*/'} - keywords contains '/bio.*/'","title":"Regular Expressions"},{"location":"queries/metadata/#adding-metadata-to-existing-odinson-documents","text":"As of PR #319 , there are some utilities and an app to help you add metadata to an Odinson document. - there is an addMetadata method in ai.lum.odinson.Document that take a sequence of Fields and adds them as metadata. - there is now a MetadataWrapper case class that can serialize itself in a compatible json format - in the extra/ subproject, there is an app ( ai.lum.odinson.extra.AddMetadataToDocuments ) that will load a set of metadata json files and a set of Document json files, and add any found metadata to the corresponding Document (as indicated by the docID in the metadata file).","title":"Adding Metadata to Existing Odinson Documents"},{"location":"queries/parent_queries/","text":"Parent queries \uf0c1 Parent queries can be applied to document metadata provided as a json of token fields, e.g.: \"metadata\": [ { \"$type\": \"ai.lum.odinson.TokensField\", \"name\": \"show\", \"tokens\": [\"Twin\", \"Peaks\"] }, { \"$type\": \"ai.lum.odinson.TokensField\", \"name\": \"actor\", \"tokens\": [\"Kyle\", \"MacLachlan\"] }, { \"$type\": \"ai.lum.odinson.TokensField\", \"name\": \"character\", \"tokens\": [\"Special\", \"Agent\", \"Dale\", \"Cooper\"] } ] Parent queries use Lucene query syntax to query metadata fields. For instance, the parent query below will limit the document collection to only the documents related to the show Twin Peaks based on the example metadata above: show: \"Twin Peaks\" The following parent query will limit the documents based on both the show and the actor: character: \"Special Agent Dale Cooper\" AND show: \"Fire Walk With Me\" See the testing suite for more query examples. Parent queries are also available in the REST API .","title":"Parent Queries"},{"location":"queries/parent_queries/#parent-queries","text":"Parent queries can be applied to document metadata provided as a json of token fields, e.g.: \"metadata\": [ { \"$type\": \"ai.lum.odinson.TokensField\", \"name\": \"show\", \"tokens\": [\"Twin\", \"Peaks\"] }, { \"$type\": \"ai.lum.odinson.TokensField\", \"name\": \"actor\", \"tokens\": [\"Kyle\", \"MacLachlan\"] }, { \"$type\": \"ai.lum.odinson.TokensField\", \"name\": \"character\", \"tokens\": [\"Special\", \"Agent\", \"Dale\", \"Cooper\"] } ] Parent queries use Lucene query syntax to query metadata fields. For instance, the parent query below will limit the document collection to only the documents related to the show Twin Peaks based on the example metadata above: show: \"Twin Peaks\" The following parent query will limit the documents based on both the show and the actor: character: \"Special Agent Dale Cooper\" AND show: \"Fire Walk With Me\" See the testing suite for more query examples. Parent queries are also available in the REST API .","title":"Parent queries"},{"location":"queries/quantifiers/","text":"Quantifiers \uf0c1 Odinson supports a full range of quantifiers which can be applied to any pattern element (i.e., tokens, graph traversals, or combinations of these). Quantifier Description Example ? indicates a pattern element is optional >amod? * matches zero or more of an element []* + matches one or more of an element (>amod [])+ {m, n} matches at least m and at most n of an element >>{2,3} Odinson also supports both greedy and lazy usage of these quantifiers. For example, [tag=/N.*/]+? will perform lazy (or reluctant) matching of nouns.","title":"Quantifiers"},{"location":"queries/quantifiers/#quantifiers","text":"Odinson supports a full range of quantifiers which can be applied to any pattern element (i.e., tokens, graph traversals, or combinations of these). Quantifier Description Example ? indicates a pattern element is optional >amod? * matches zero or more of an element []* + matches one or more of an element (>amod [])+ {m, n} matches at least m and at most n of an element >>{2,3} Odinson also supports both greedy and lazy usage of these quantifiers. For example, [tag=/N.*/]+? will perform lazy (or reluctant) matching of nouns.","title":"Quantifiers"},{"location":"queries/queries/","text":"Odinson queries \uf0c1 Odinson supports two main types of query patterns, basic queries and event queries. Each type of query can hop between surface representations and graph traversals (e.g., syntactic dependencies) using a combination of token constraints and graph traversals . A basic query minimally contains a token pattern (one or more token constraints), but optionally can also include graph traversals and additional token patterns. Example: girl >nmod_from Ipanema An event query requires a trigger , a token pattern that indicates a possible match, and can have arguments, i.e., patterns anchored on the trigger. A simple example of this would be having a certain verb as a trigger (e.g., \"cause\"), and looking for the subject and object of the verb to serve as the agent and the theme of the event. In addition to the two types of queries above, aimed at matching patterns within the body of a document, Odinson supports parent queries , which filter the document collection based on information in the document metadata before an odinson query executes. Parent queries use Lucene query syntax .","title":"Overview"},{"location":"queries/queries/#odinson-queries","text":"Odinson supports two main types of query patterns, basic queries and event queries. Each type of query can hop between surface representations and graph traversals (e.g., syntactic dependencies) using a combination of token constraints and graph traversals . A basic query minimally contains a token pattern (one or more token constraints), but optionally can also include graph traversals and additional token patterns. Example: girl >nmod_from Ipanema An event query requires a trigger , a token pattern that indicates a possible match, and can have arguments, i.e., patterns anchored on the trigger. A simple example of this would be having a certain verb as a trigger (e.g., \"cause\"), and looking for the subject and object of the verb to serve as the agent and the theme of the event. In addition to the two types of queries above, aimed at matching patterns within the body of a document, Odinson supports parent queries , which filter the document collection based on information in the document metadata before an odinson query executes. Parent queries use Lucene query syntax .","title":"Odinson queries"},{"location":"queries/strings/","text":"String rules and special characters \uf0c1 There are four places where Odinson needs to match a string: 1. for the name of the token field (e.g., tag or lemma ) 2. when matching the surface content of a token 3. when matching the label of a graph edge (e.g., syntactic dependency label) 4. when patching a plain surface pattern (e.g., dog when not wrapped in square brackets, as this is a special case of 2, above) In all those cases, you need to use single or double quotes (they are equivalent in Odinson). Inside the quotes, the rules for string escaping are applied (i.e., \\n will be a newline, though you wouldn't encounter a newline in a token under normal circumstances). However, if the string is a valid java identifier, then you don't need to quote it. A valid java identifier begins with a letter or underscore and is followed by zero or more letters, digits, or underscores. If your string is of this format, it does not need to be quoted. Additionally, if you're matching anything other than the default token (i.e., if you're using the [...] or >... notation), then we add two additional options to that list, : and - . That is, [chunk=B-NP] does not need any quotes, and neither does >nmod:poss . (You can quote it if you want, knock yourself out!) As mentioned above, these rules do not apply to the default word in surface pattens, so you do need quotes in this pattern: \"3:10\" to Yuma Further, you can also use regular expressions anywhere Odinson supports strings. For example, you can specify >/nmod_.*/ .","title":"Strings"},{"location":"queries/strings/#string-rules-and-special-characters","text":"There are four places where Odinson needs to match a string: 1. for the name of the token field (e.g., tag or lemma ) 2. when matching the surface content of a token 3. when matching the label of a graph edge (e.g., syntactic dependency label) 4. when patching a plain surface pattern (e.g., dog when not wrapped in square brackets, as this is a special case of 2, above) In all those cases, you need to use single or double quotes (they are equivalent in Odinson). Inside the quotes, the rules for string escaping are applied (i.e., \\n will be a newline, though you wouldn't encounter a newline in a token under normal circumstances). However, if the string is a valid java identifier, then you don't need to quote it. A valid java identifier begins with a letter or underscore and is followed by zero or more letters, digits, or underscores. If your string is of this format, it does not need to be quoted. Additionally, if you're matching anything other than the default token (i.e., if you're using the [...] or >... notation), then we add two additional options to that list, : and - . That is, [chunk=B-NP] does not need any quotes, and neither does >nmod:poss . (You can quote it if you want, knock yourself out!) As mentioned above, these rules do not apply to the default word in surface pattens, so you do need quotes in this pattern: \"3:10\" to Yuma Further, you can also use regular expressions anywhere Odinson supports strings. For example, you can specify >/nmod_.*/ .","title":"String rules and special characters"},{"location":"queries/token_constraints/","text":"Token constraints \uf0c1 The simplest possible Odinson patterns consist of a single token constraint. A token constraint specifies what must be true of a token in order for it to be a valid extraction. These constraints are limited only by what you include in your index. For example, we commonly include part of speech tag, named entity information (NER), and chunk. Example \uf0c1 If you write a query such as this: dog Odinson will look for any occurrence of the word dog . Unless specified otherwise, this will be case-insensitive and will normalize accents and Unicode characters. That is, this pattern will match: dog , DoG , and d\u00f6g . Using the token fields \uf0c1 If you want to write a token constraint that uses the indexed fields, you can use this format: [tag=/N.*/] This pattern will match any token in a document whose part of speech tag begins with \"N\". Here, tag is the specified field of the constraint. Any token constraint with an unspecified field (e.g., dog in the above example) will be matched against the norm field, which is the default. That is, dog is equivalent to [norm=dog] . The field names are specified here , and for most use cases should not need to be modified. Operators for token constraints \uf0c1 You can combine and nest constraints for a token using operators: & (for AND), | (for OR) and parentheses. For example, the following would match a word whose tag starts with \"N\" and which is also tagged as an organization using NER or is a proper noun. [tag=/N.*/ & (entity=ORGANIZATION | tag=NNP)] Wildcards \uf0c1 Odinson supports wildcards for token constraints. Specifically, - [] : any token Quantifiers \uf0c1 Like any other pattern component, token constraints (as well as these wildcards) can be combined with quantifiers , e.g., [chunk=B-NP] [chunk=I-NP]*","title":"Token Constraints"},{"location":"queries/token_constraints/#token-constraints","text":"The simplest possible Odinson patterns consist of a single token constraint. A token constraint specifies what must be true of a token in order for it to be a valid extraction. These constraints are limited only by what you include in your index. For example, we commonly include part of speech tag, named entity information (NER), and chunk.","title":"Token constraints"},{"location":"queries/token_constraints/#example","text":"If you write a query such as this: dog Odinson will look for any occurrence of the word dog . Unless specified otherwise, this will be case-insensitive and will normalize accents and Unicode characters. That is, this pattern will match: dog , DoG , and d\u00f6g .","title":"Example"},{"location":"queries/token_constraints/#using-the-token-fields","text":"If you want to write a token constraint that uses the indexed fields, you can use this format: [tag=/N.*/] This pattern will match any token in a document whose part of speech tag begins with \"N\". Here, tag is the specified field of the constraint. Any token constraint with an unspecified field (e.g., dog in the above example) will be matched against the norm field, which is the default. That is, dog is equivalent to [norm=dog] . The field names are specified here , and for most use cases should not need to be modified.","title":"Using the token fields"},{"location":"queries/token_constraints/#operators-for-token-constraints","text":"You can combine and nest constraints for a token using operators: & (for AND), | (for OR) and parentheses. For example, the following would match a word whose tag starts with \"N\" and which is also tagged as an organization using NER or is a proper noun. [tag=/N.*/ & (entity=ORGANIZATION | tag=NNP)]","title":"Operators for token constraints"},{"location":"queries/token_constraints/#wildcards","text":"Odinson supports wildcards for token constraints. Specifically, - [] : any token","title":"Wildcards"},{"location":"queries/token_constraints/#quantifiers","text":"Like any other pattern component, token constraints (as well as these wildcards) can be combined with quantifiers , e.g., [chunk=B-NP] [chunk=I-NP]*","title":"Quantifiers"}]}